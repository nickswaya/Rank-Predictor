{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f153955522cc68e74ca65e6cd89b55cb81ff123da8886c6f001578a93a739a7e"
   }
  },
  "interpreter": {
   "hash": "f153955522cc68e74ca65e6cd89b55cb81ff123da8886c6f001578a93a739a7e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from random import randrange, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from helper import *\n",
    "import math\n",
    "import logging\n",
    "from retrying import retry\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from itertools import islice\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostClassifier,RandomForestClassifier)\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.model_selection as cv\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (12, 8)\n",
    "plt.style.use('ggplot')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "d['list'][0].keys()\n",
    "replay_ids = []\n",
    "for i in range(200):\n",
    "    replay_ids.append(d['list'][i]['id'])\n",
    "replay_ids[0:5]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3e8c8524eb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreplay_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreplay_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreplay_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_extra_element(row):\n",
    "    if 'goals_against_while_last_defender' in row:\n",
    "        del row['goals_against_while_last_defender']\n",
    "    return row\n",
    "\n",
    "\n",
    "def create_frame_from_json(json_stats, rank_target):\n",
    "    df = pd.DataFrame.from_dict(json_stats)\n",
    "\n",
    "    df['core'] = df.stats.apply(lambda x:x['core'])\n",
    "    df['boost'] = df.stats.apply(lambda x:x['boost'])\n",
    "    df['movement'] = df.stats.apply(lambda x:x['movement'])\n",
    "    df['positioning'] = df.stats.apply(lambda x:x['positioning'])\n",
    "    df['demo'] = df.stats.apply(lambda x:x['demo'])\n",
    "    \n",
    "\n",
    "    rank_keys = list(df.iloc[0]['rank'].keys())\n",
    "    camera_keys = list(df.iloc[0]['camera'].keys())\n",
    "    core_stats_keys = list(df.iloc[0]['stats']['core'].keys())\n",
    "    boost_keys = list(df.iloc[0]['stats']['boost'].keys())\n",
    "    movement_keys = list(df.iloc[0]['stats']['movement'].keys())\n",
    "    positioning_keys = ['avg_distance_to_ball', 'avg_distance_to_ball_possession',\n",
    "        'avg_distance_to_ball_no_possession', 'avg_distance_to_mates',\n",
    "        'time_defensive_third', 'time_neutral_third', 'time_offensive_third',\n",
    "        'time_defensive_half', 'time_offensive_half', 'time_behind_ball',\n",
    "        'time_infront_ball', 'time_most_back', 'time_most_forward',\n",
    "        'time_closest_to_ball', 'time_farthest_from_ball',\n",
    "        'percent_defensive_third', 'percent_offensive_third',\n",
    "        'percent_neutral_third', 'percent_defensive_half',\n",
    "        'percent_offensive_half', 'percent_behind_ball', 'percent_infront_ball',\n",
    "        'percent_most_back', 'percent_most_forward', 'percent_closest_to_ball',\n",
    "        'percent_farthest_from_ball']\n",
    "    demo_keys = list(df.iloc[0]['stats']['demo'].keys())\n",
    "\n",
    "    df[rank_keys] = pd.json_normalize(df['rank'])\n",
    "    df[camera_keys] = pd.json_normalize(df['camera'])\n",
    "    df[core_stats_keys] = pd.json_normalize(df['core'])\n",
    "    df[boost_keys] = pd.json_normalize(df['boost'])\n",
    "    df[movement_keys] = pd.json_normalize(df['movement'])\n",
    "    df[demo_keys] = pd.json_normalize(df['demo'])\n",
    "\n",
    "    df['positioning'] = df.stats.apply(lambda x: remove_extra_element(x['positioning']))\n",
    "    df[positioning_keys] = pd.json_normalize(df['positioning'])\n",
    "    if rank_target == 'tier_div':\n",
    "        df['rank_target'] = df['tier'] * 4 + df['division']\n",
    "        df = df.drop(['camera', 'stats', 'camera', 'rank', 'core', 'boost', 'movement', 'positioning', 'demo', 'start_time', 'end_time', 'name', 'id', 'car_id', 'tier', 'division'], axis=1)\n",
    "        return df\n",
    "    if rank_target == 'tier':\n",
    "        df['rank_target'] = df['id'] \n",
    "        df = df.drop(['camera', 'stats', 'camera', 'rank', 'core', 'boost', 'movement', 'positioning', 'demo', 'start_time', 'end_time', 'name', 'id', 'car_id', 'tier', 'division'], axis=1)\n",
    "        return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_all_ids():\n",
    "    season_lst = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','f1', 'f2', 'f3', 'f4']\n",
    "    replay_ids = []\n",
    "    for i in range(len(season_lst)):\n",
    "        r = requests.get(f'https://ballchasing.com/api/replays?player-name=LebaneseNinja&player-id=steam:76561198068157523&count=200&playlist=ranked-standard&season={season_lst[i]}', headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "        d = r.json()\n",
    "        rank_ids = []\n",
    "        for i in range(len(d['list'])):\n",
    "            rank_ids.append(d['list'][i]['id'])\n",
    "        replay_ids.append(rank_ids)\n",
    "    return replay_ids\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_stats_from_replays(replay_ids):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    logging.basicConfig(filename = 'logging_info.log', level=logging.INFO, force=True, format=log_fmt)\n",
    "\n",
    "    orange_ranks = []\n",
    "    orange_stats = []\n",
    "    blue_ranks = []\n",
    "    blue_stats = []\n",
    "    pull_num = 0\n",
    "    for replay_id in replay_ids:\n",
    "        replay_stats = requests.get(f'https://ballchasing.com/api/replays/{replay_id}', headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "        test_stats = replay_stats.json()\n",
    "        try:\n",
    "            ranks = [test_stats['orange']['players'][player]['rank']['name'] for player in range(3)]\n",
    "            orange_ranks.append(ranks)\n",
    "            stats = [test_stats['orange']['players'][player] for player in range(3)]\n",
    "            orange_stats.append(stats[0])\n",
    "            orange_stats.append(stats[1])\n",
    "            orange_stats.append(stats[2])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            ranks = [test_stats['blue']['players'][player]['rank']['name'] for player in range(3)]\n",
    "            blue_ranks.append(ranks)\n",
    "            stats = [test_stats['blue']['players'][player] for player in range(3)]\n",
    "            blue_stats.append(stats[0])\n",
    "            blue_stats.append(stats[1])\n",
    "            blue_stats.append(stats[2])\n",
    "        except:\n",
    "            pass\n",
    "        pull_num += 1\n",
    "        logger.info(f'stats extracted for pull num {pull_num}')\n",
    "        \n",
    "    return blue_stats, orange_stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def combine_clean_dfs(blue_df, orange_df, car_dummies):\n",
    "    combined_dfs = blue_df.append(orange_df)\n",
    "    combined_dfs['mvp'] = combined_dfs.mvp.apply(lambda x: 1 if x==True else 0)\n",
    "\n",
    "    if car_dummies == False:\n",
    "        combined_dfs = combined_dfs.drop('car_name', axis=1)\n",
    "        return combined_dfs\n",
    "    if car_dummies == True:\n",
    "        dummies = pd.get_dummies(combined_dfs.car_name, drop_first=True)\n",
    "        combined_concat_w_dummies = pd.concat([combined_dfs, dummies], axis=1)\n",
    "        combined_concat_w_dummies = combined_concat_w_dummies.drop('car_name', axis=1)\n",
    "        return combined_concat_w_dummies"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_random_ids():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    logging.basicConfig(filename = 'logging_info.log', level=logging.INFO, force=True, format=log_fmt)\n",
    "    logger.info('starting pull')\n",
    "    minmax_rank_lst = [['bronze-1', 'bronze-3'], ['silver-1', 'silver-3'], ['gold-1', 'gold-3'], ['platinum-1', 'platinum-3'], ['diamond-1', 'diamond-3'], ['champion-1', 'champion-3'], ['champion-3', 'grand-champion']]\n",
    "    replay_ids = []\n",
    "    ids_pulled = []\n",
    "    for i in range(len(minmax_rank_lst)):\n",
    "        logger.info(f'pulling min_rank={minmax_rank_lst[i][0]}, max_rank = {minmax_rank_lst[i][1]}')\n",
    "        r = requests.get(f'https://ballchasing.com/api/replays?&count=200&playlist=ranked-standard&min-rank={minmax_rank_lst[i][0]}&max-rank={minmax_rank_lst[i][1]}', headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "        d = r.json()\n",
    "        rank_ids = []\n",
    "        count = d['count']\n",
    "        logger.info(f'count = {count}')\n",
    "        for i in range(len(d['list'])):\n",
    "            rank_ids.append(d['list'][i]['id'])\n",
    "        next_link = d['next']\n",
    "        if i == 6 or i == 5:\n",
    "            logger.info('i == 5 or 6, moving on')\n",
    "            replay_ids.append(rank_ids)\n",
    "            continue \n",
    "\n",
    "        logger.info('starting second pull')\n",
    "        r = requests.get(next_link, headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "        d = r.json()\n",
    "        for i in range(len(d['list'])):\n",
    "            rank_ids.append(d['list'][i]['id'])\n",
    "        next_link = d['next']\n",
    "\n",
    "        logger.info('starting third pull')\n",
    "        r = requests.get(next_link, headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "        d = r.json()\n",
    "        for i in range(len(d['list'])):\n",
    "            rank_ids.append(d['list'][i]['id'])\n",
    "        logger.info('rank pull complete.. appending results and moving to next rank') \n",
    "        replay_ids.append(rank_ids)\n",
    "\n",
    "    flat_list = [item for sublist in replay_ids for item in sublist]\n",
    "    return flat_list\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('full_nodummies.csv')\n",
    "df = df.dropna()\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "y = df.pop('rank_target')\n",
    "X = df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, shuffle=True)\n",
    "\n",
    "kf = KFold(10)\n",
    "rf_pipe = Pipeline([('classifier', GradientBoostingRegressor())])\n",
    "rf_grid_params = {'classifier__max_features': ['sqrt', 'log2'], 'classifier__n_estimators': [900, 1000, 1100]}\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_grid_params, cv=kf, scoring = 'neg_mean_absolute_error')\n",
    "rf_grid.fit(X_train, y_train) \n",
    "print(\"Gradient Boosting Mean Absolute Error: {}\".format(rf_grid.score(X_test, y_test)))\n",
    "\n",
    "'''\n",
    "best estimator : [('classifier',\n",
    "                 GradientBoostingRegressor(max_features='sqrt',\n",
    "                                           n_estimators=900))])\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "axs = axs.flatten()\n",
    "axs[0].hist(y_train)\n",
    "axs[0].set_title('y_train rank distribution')\n",
    "axs[0].set_ylabel('count')\n",
    "axs[0].set_xlabel('rank target')\n",
    "axs[1].hist(y_test)\n",
    "axs[1].set_title('y_test rank distribution')\n",
    "axs[1].set_ylabel('count')\n",
    "axs[1].set_xlabel('rank target');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "keras = tf.keras\n",
    "\n",
    "NN_model = keras.models.Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(keras.layers.Dense(100, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(keras.layers.Dense(90, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(keras.layers.Dense(90, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(keras.layers.Dense(90, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(keras.layers.Dense(90, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(keras.layers.Dense(45, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(keras.layers.Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split = 0.2)\n",
    "'''\n",
    "224/224 [==============================] - 0s 831us/step - loss: 7.2956 - mean_absolute_error: 7.2956 - val_loss: 7.2235 - val_mean_absolute_error: 7.2235\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def test_all():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    logging.basicConfig(filename = 'logging_info.log', level=logging.INFO, force=True, format=log_fmt)\n",
    "    logger.info('starting pull')\n",
    "    minmax_rank_lst = [['champion-1', 'champion-3'], ['champion-3', 'grand-champion']]\n",
    "    replay_ids = []\n",
    "    ids_pulled = []\n",
    "    for i in range(len(minmax_rank_lst)):\n",
    "        logger.info(f'pulling min_rank={minmax_rank_lst[i][0]}, max_rank = {minmax_rank_lst[i][1]}')\n",
    "        r = requests.get(f'https://ballchasing.com/api/replays?&count=200&playlist=ranked-standard&min-rank={minmax_rank_lst[i][0]}&max-rank={minmax_rank_lst[i][1]}', headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "        d = r.json()\n",
    "        rank_ids = []\n",
    "        count = d['count']\n",
    "        next_link = d['next']\n",
    "        logger.info(f'count = {count}')\n",
    "        for pull in range(math.floor(count/200)-2):\n",
    "            logger.info(f'starting pull number {pull+1}')\n",
    "            r = requests.get(next_link, headers={'Authorization': 'gMXy4BUhXt0OQhc37kJV5KP0GUyLzhJeZhogpa94'})\n",
    "            d = r.json()\n",
    "            try:\n",
    "                for i in range(len(d['list'])):\n",
    "                    rank_ids.append(d['list'][i]['id'])\n",
    "            except:\n",
    "                pass\n",
    "            next_link = d['next']\n",
    "        logger.info('rank pull complete.. appending results, saving series and moving to next rank') \n",
    "        replay_ids.append(rank_ids)\n",
    "        match_ids_tosave = pd.Series(replay_ids)\n",
    "        match_ids_tosave.to_csv('test_pull_all_ids.csv')\n",
    "    flat_list = [item for sublist in replay_ids for item in sublist]\n",
    "    return flat_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "up_to_d1 = pd.read_csv('up_to_diamond1.csv')\n",
    "d1uptochamp = pd.read_csv('diamond1_uptochamp1.csv')\n",
    "champtogc = pd.read_csv('champ_to_gc.csv')\n",
    "\n",
    "\n",
    "d1uptochampa = d1uptochamp.iloc[0]['0']\n",
    "da = d1uptochampa.split(',')\n",
    "da = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in da]\n",
    "\n",
    "\n",
    "champtogca = champtogc.iloc[0]['0']\n",
    "gca = champtogca.split(',')\n",
    "gca = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in gca]\n",
    "\n",
    "champtogcb = champtogc.iloc[1]['0']\n",
    "gcb = champtogcb.split(',')\n",
    "gcb = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in gcb]\n",
    "\n",
    "\n",
    "up_to_d1_a = up_to_d1.iloc[0]['0']\n",
    "a = up_to_d1_a.split(',')\n",
    "a = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in a]\n",
    "\n",
    "up_to_d1_b = up_to_d1.iloc[1]['0']\n",
    "b = up_to_d1_b.split(',')\n",
    "b = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in b]\n",
    "\n",
    "up_to_d1_c = up_to_d1.iloc[2]['0']\n",
    "c = up_to_d1_c.split(',')\n",
    "c = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in c]\n",
    "\n",
    "up_to_d1_d = up_to_d1.iloc[3]['0']\n",
    "d = up_to_d1_d.split(',')\n",
    "d = [i.replace('\"', '').replace('[', '').replace('\"\"', '').replace('\\'', '').replace(']', '') for i in d]\n",
    "\n",
    "res = []\n",
    "res.append(a)\n",
    "res.append(b)\n",
    "res.append(c)\n",
    "res.append(d)\n",
    "res.append(da)\n",
    "res.append(gca)\n",
    "res.append(gcb)\n",
    "\n",
    "flat_list = [item for sublist in res for item in sublist]\n",
    "res1 = []\n",
    "for i in flat_list:\n",
    "    if i not in res1:\n",
    "        res1.append(i)\n",
    "res1 = [i.replace(' ', '') for i in res1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "'27,095'\n",
    "blue_stats, orange_stats = get_stats_from_replays(res1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "blue_df = create_frame_from_json(blue_stats, 'tier_div')\n",
    "orange_df = create_frame_from_json(orange_stats, 'tier_div')\n",
    "full_df = combine_clean_dfs(blue_df, orange_df, car_dummies=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "full_df = full_df.dropna()\n",
    "full_df.to_csv('final_full_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "y = full_df.pop('rank_target')\n",
    "X = full_df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, shuffle=True)\n",
    "\n",
    "kf = KFold(10)\n",
    "rf_pipe = Pipeline([('classifier', GradientBoostingRegressor())])\n",
    "rf_grid_params = {'classifier__max_features': ['sqrt', 'log2'], 'classifier__n_estimators': np.arange(900, 1200, 50)}\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_grid_params, cv=kf, scoring = 'neg_mean_absolute_error')\n",
    "rf_grid.fit(X_train, y_train) \n",
    "print(\"Gradient Boosting Mean Absolute Error: {}\".format(rf_grid.score(X_test, y_test)))\n",
    "\n",
    "'''\n",
    "best estimator : [('classifier',\n",
    "                 GradientBoostingRegressor(max_features='sqrt',\n",
    "                                           n_estimators=900))])\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "np.arange(900, 1200, 50)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 900,  950, 1000, 1050, 1100, 1150])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}